\documentclass[a4paper]{jpconf}
\usepackage{graphicx}
\begin{document}
\title{Comparing SyCL data transfer strategies for tracking use cases }

% \author{Jacky Mucklow}
% \address{Production Editor, \jpcs, \iopp, Dirac House, Temple Back, Bristol BS1~6BE, UK}
% \ead{jacky.mucklow@iop.org}

\author{S Joube$^{1, 2}$, H Grasland$^1$,
D Chamont$^1$, and E Brunet$^2$}
\address{$^1$ Université Paris-Saclay, CNRS/IN2P3, IJCLab, 91405 Orsay, France}
\address{$^2$ Samovar, Télécom SudParis, Institut Polytechnique de Paris, rue Charles Fourier, 91011 Evry, France}
\ead{joube@ijclab.in2p3.fr}

\begin{abstract}
\textbf{L'abstract sera refait à la fin. [Présentation du contexte général]} The increased use of accelerators for scientific computing, together with the increased variety of hardware involved, induces a need for performance portability between at least CPUs (which largely dominate WLCG infrastructure) and GPUs (which are quickly emerging as an architecture of choice for online data processing and HPC centers). In the C/C++ community, OpenCL was a low level first step, which is now superseded by SyCL for C++ use cases, as far as Khronos-backed open standards go. \textbf{[Présentation de l’angle d’étude du contexte]} A key factor in GPU performance is the balance between data transfer rates, on-device computational efficiency and utilization by incoming jobs. The aim of this work is to compare the ease of programming and performance portability of the three data transfer strategies of SyCL (Accessors and buffers, Unified Shared Memory (USM) with implicit transfers and USM with explicit transfers), over several hardware targets, including a discrete GPU with dedicated VRAM and an integrated GPU that shares its memory with the CPU. \textbf{[Présentation des résultats obtenus]} Ne pas oublier de parler de SparseCCL. \textbf{[Présentation des conclusions principales]}. But : raconter ce que j'ai fait sans spoiler la conclusion. (qu'est-ce que j'ai fait et pourquoi)
\end{abstract}

\section{TODO - choses qui seront bientôt faites}
- Parler de ACTS dans l'introduction \linebreak
- Rajouter dans la légende la gradeur des tableaux (ms) et mettre ms entre parenthèses dans les graphiques. \linebreak
- Ne pas répérter les choses, faire des références. \linebreak
- Pas grave d'avoir une biblio nulle pour la première version, ça fera des choses simples à relever pour le rapporteur. \linebreak
- Conclusion : reprendre les principaux résultats et dire ce qui pourrait être fait plus tard "further research could focus on ...". Dans la suite du travail, aussi : les autres use-case du tracking. \linebreak
- Introduction : voir les notes de l'intro \linebreak
- Finir SparseCCL (ping David et Hadrien quand c'est fini) + avancer conclu, intro et références (ne pas passer trop de temps sur les références, c'est pas grave si c'est pas bon). L'abstract pourra être fait super rapidement par David et Hadrien si besoin. \linebreak

\section{Introduction}
\textbf{[Rapide mise en contexte générale]} \textbf{[Problématique]} \textbf{[Présentation de SyCL]} SyCL is a royalty-free, cross-platform abstraction layer that enables code for heterogeneous processors to be written using standard ISO C++ with the host and kernel code for an application contained in the same source file. \textbf{[Présentation du hardware utilisé]} Every benchmark was ran on the same architecture : a Debian 11.0 stable based server with 512 GB DDR4 at 2400 MHz, an AMD EPYC 7502 CPU with 32 cores, an NVidia Quadro RTX 5000 having 16 GB GDDR6 and a theoretical memory bandwidth of 448 GB/s, a PCIe 3.0 x16 bus with a theoretical bandwidth of 16 GB/s yet an observed bandwidth of only 12 GB/s. (+réf possible même si pas publication solide ? (à mon avis non)) \textbf{[Présentation du logiciel utilisé]} + parler rapidement de ACTS (contexte, motivation, application)

% TODO: Ajouter ref + version hipSYCL (et bien insister que c'est hipSYCL, performances des buffers/acesseurs potentiellement fortement dépendantes de l'implémentation utilisée)

\section{Microbenchmark}
A simple microbenchmark was made to give a first approximation of the relative performance of the SyCL memory models on a single hardware architecture, and to compare it with what is theoretically expected. This microbenchmark is a simple reduction on GPU, heavily I/O bound, computing partial sums of about 1.6 billion randomly generated 4-byte floats without synchronisation between threads. Two kernels were launched each time, one after the other, to identify what is done in a lazy way.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{ubench_explicit4.png}

    %\begin{center} <- décommenter pour prendre plus de place et du coup aérer plus
    \begin{tabular}{||c c c c c c||} 
    \hline
    & alloc & copy & ker\textsubscript{1} & ker\textsubscript{2} & free \\ [0.5ex]
    \hline\hline
    device & 7 & 671 & 17 & 17 & 15 \\
    \hline
    shared & 0.1 & 981 & 24 & 32 & 169 \\
    \hline
    host & 1935 & 404 & 525 & 530 & 1014 \\
    \hline
    accessors & 0.0 & 0.02 & 701 & 17 & 5 \\
    \hline
    \end{tabular}
    %\end{center} <- décommenter pour prendre plus de place et du coup aérer plus
    
    \caption{Elapsed time for the main stages of the microbenchmark, using explicit copy from host native memory to SyCL Unified Shared Memory on the one hand and the buffers/accessors model on the other. \textit{alloc} represents the SyCL USM memory allocation and buffers declaration. During the \textit{copy} step data is explicitly copied to the SyCL memory for USM, and buffers are linked to the data they will manage on the host. \textit{kernel 1} stands for the first kernel. \textit{kernel 2} is launched just after the first one has completed, and works with the same dataset. \textit{free} is the final freeing of the SyCL memory. }
    \label{fig:microbenchmark}
\end{figure}

\textit{USM device}, \textit{USM shared} and \textit{USM host} curves shown in figure \ref{fig:microbenchmark} are made with an explicit SyCL memory copy (sycl::memcpy) from host native memory to the Unified Shared Memory handled by SyCL. The input size for the computation is 6 GiB, while the output size is 48 MiB. The copy for USM device takes 671 ms to complete, accounting for an observed speed of about 10 GB/s if we consider the overheads duration to be insignificant compared to the copy duration itself. Its kernel is processed in 17ms, leading to a deduced memory bandwidth for the GPU of about 360 GB/s, relatively close to the theoretical 448 GB/s of this GPU, as the microbenchmark is heavily memory-bound. Given that the USM host kernel only takes about 520 ms (12.4 GB/s), its usage seems to fit the single-access-to-data use-case if no copy is needed and data is handled and allocated through sycl::malloc\_host instead of the regular malloc. This avoids paying for the copy overhead of USM device, provided that the SyCL host memory is heavily reused to compensate for its huge allocation and deallocation costs. \medbreak 

The accessors-buffers curve has a very similar performance profile to USM device, except the memory copy is performed in a lazy way the first time data is accessed (GPU kernel run 1). Its first kernel takes about 701ms, roughly equivalent to the 671ms of SyCL explicit copy plus the 17ms of kernel time taken by USM device. Its allocation time is nearly zero as only the sycl::buffer wrapper objects are created. The "copy" time also is very small as it only consists of passing a pointer to the sycl::buffer. As said before, the data is only copied when needed, that is during the first kernel run. \medbreak 

USM shared also has a very low allocation time, but a slightly higher copy time compared to USM device. The real allocations on device are done during the copy phase. The internal logic needed by USM shared also has a significant cost paid on the free SyCL memory step. \medbreak 

Here is short conclusion for this section. The accessors/buffers approach is the latest and most elegant way of managing data, as it relies on a graph of dependency between tasks, leaving the logic of data dependencies entirely up to the SyCL API. It is comparatively lightweight and very efficient, provided data already appears in 1-dimensional arrays, that can easily be passed to sycl::buffers as pointers. As pointer-based methods goes, USM device is the most efficient way of dealing with a heavily memory-bound program, with the same constraints on data format than the accessors/buffers technique: to make an efficient sycl::memcpy, data must be stored in big 1-dimensional arrays. The one downside to USM device is that data cannot be larger than the device memory, and has to be manually fragmented if too large, an issue that no other method has. There is no limit to data size for USM host, shared and the accessors/buffers, except for the physical available memory on the host. USM host has a huge allocation and deallocation time, and may be only useful for data accessed only once by the device and only handled by SyCL USM host memory, never copied from a regular malloc/new memory. \medbreak 

\begin{figure}[h]
    \centering
    
    \includegraphics[width=0.9\textwidth]{ubench_write3.png}
    
    \begin{center}
    \begin{tabular}{||c c c c c c||} 
    \hline
    & alloc & copy/write & ker\textsubscript{1} & ker\textsubscript{2} & free \\ [0.5ex]
    \hline\hline
    s. direct & 0.1 & 1659 & 1383 & 31 & 654 \\
    \hline
    s. copy & 0.1 & 981 & 24 & 32 & 169 \\
    \hline
    \end{tabular}
    \end{center}
    
    \caption{Elapsed time for the main stages of the microbenchmark, comparing an explicit SyCL copy from host native memory to USM shared memory, and a direct access to USM shared memory without the SyCL explicit copy. The meaning of the labels is exactly the same as with Figure \ref{fig:microbenchmark} except data is copied to the SyCL memory during the \textit{copy/write} step for USM shared copy and directly written to for USM shared diect.}
    \label{fig:microbenchmark_write}
\end{figure}

As seen before, the main advantages of USM shared and host over USM device are that they can handle data way bigger that the device memory and that data can be accessed both from the host and the device. Figure \ref{fig:microbenchmark} shows an explicit copy from host native memory to Unified Shared Memory. Figure \ref{fig:microbenchmark_write} shows USM shared behaviour when instead of being copied with sycl::memcpy, data is directly written to the USM shared memory from the host. For readability USM host was not added to the graph. It has the exact same performance profile, when explicitly copied or directly written to. \medbreak 

Figure \ref{fig:microbenchmark_write} shows that USM shared direct pays a huge overhead: the write time to the SyCL memory is multiplied by 1.6 and the first kernel time by 55, while the second kernel time is not affected. By profiling the execution of this program, we found that the memory copy still goes on in an asynchronous way, until the first kernel has completed. The automatic data migration proves to be highly inefficient in this case, as SyCL does not know we need to access memory in a respectively read-only and write-only mode. This results in a huge number of page faults as data is accessed from the device, that only finishes during the first kernel run, once the whole dataset has been accessed by the kernel and data has been entirely moved to the device memory. The second kernel run has a very close performance profile to the explicit sycl::memcpy. Here, the whole input and output data buffers could be stored entirely in the device memory as the comparison with USM device would not have been possible otherwise. \medbreak 

The \textit{free SyCL} stage shows that the synchronisation mechanisms used by USM shared, together with the API not knowing whether we wanted a \textit{read-only} or a \textit{write-only} buffer, makes deallocation very expensive. \medbreak 


\section{SparseCCL}

SparseCCL is an algorithm specifically designed for HEP tracking. It is in charge of the very first step of the track reconstruction: the connected component labeling. Its memory access pattern are very different from the ones of our microbenchmark. It also represents a much more realistic usage of the SyCL API. This section still aims to compare the relative performance of the various Unified Shared Memory methods, together with the accessors/buffers technique.

\subsection{SparseCCL with flat arrays}

First we chose to structure the memory in a relatively device-friendly fashion. Data is flattened in big arrays, easily copyable to the Unified Shared Memory on the one hand and making the creation of sycl::buffer wrappers objects trivial by giving them a pointer to the contiguous flat arrays allocated on the host. \medbreak 

Because the initial data is not already flattened into an array, the allocation (alloc) step depicted in Figure \ref{fig:sparseccl_flat} is quite different from the one shown on \ref{fig:microbenchmark}. For USM device and the buffers/accessors, a host memory allocation is needed to flatten the data and then explicitly copy it to the USM device memory, or be passed as a pointer to a sycl::buffer. Therefore, the alloc phase displays the host memory allocation for both USM device and the buffers/accessors, in addition to the device memory allocation for USM device and the (still insignificant) creation time of sycl::buffer wrappers objects. The fill phase represents the memory being filled with the data of interest. Data is put into the flat host memory for USM device and the buffers/accessors in order to be copied later, while data is directly written to the USM shared memory. During the \textit{copy+kernel} phase memory is copied from host to USM device and then the kernel starts, while the kernel starts directly for USM shared and the accessors/buffers as SyCL is in charge of the implicit data migration. Finally, memory is freed during the free step. The need to allocate and fill some host memory for the buffers/accessors and for USM device greatly impairs performance, making USM shared much more competitive than previously depicted. For readability, USM host has not been shown in Figure \ref{fig:sparseccl_flat} as its kernel takes an extremely long time to complete, due to many accesses to the same data. Instead of accessing device local memory, or even caches, an I/O request on the PCIe bus is issued each time the kernel needs to access memory, leading to an extremely poor kernel speed.

\begin{figure}[h]
    \centering
    %\includegraphics[width=0.8\textwidth]{SparseCCL_flat.png}
    \includegraphics[width=0.8\textwidth]{sparseccl_flat_sandor_ld100_reduced.png}
    \medbreak 
    \begin{tabular}{||c c c c c c||} 
    \hline
    & alloc & fill & kernel & free & sum \\ [0.5ex]
    \hline\hline
    device & 685 & 575 & 332 & 311 & 1903 \\
    \hline
    shared & 0.09 & 838 & 452 & 266 & 1556 \\
    \hline
    accessors & 276 & 574 & 392 & 13 & 1255 \\
    \hline
    host & 724 & 563 & 11505 & 406 & 13198 \\
    \hline
    \end{tabular}
    \caption{Elapsed time for the main stages of the SparseCCL memory management. The necessary allocations are made during the alloc phase. Data is flattened into big arrays on the fill step. The kernel+copy phase is where the explicit copy and kernel takes place for USM device, while only the kernel runs for USM shared and the accessors/buffers. Finally, the memory is freed during the free phase.}
    \label{fig:sparseccl_flat}
\end{figure}

\subsection{SparseCCL: flat arrays versus pointer graph}

Flattering data to make it usable by any device is not the only option. Thanks to the SyCL USM shared and USM host models, pointers can be used both on device and on the host. It is thus possible for any CPU-only program to keep the same data structure and make it run on any device supported by a SyCL implementation and backend, provided some changes are made to the computation algorithms to fully take advantage of the device in use. To make the data accessible by the host and the device, it is only necessary to change the regular c++ allocator such as malloc or new, to a sycl::malloc\_host or sycl::malloc\_shared allocator. \medbreak 

The performance implications of such a change are shown in Figure \ref{fig:sparseccl_ptr}. The flat version only requires 4 arrays to be allocated with SyCL USM shared, host or with a regular malloc : two arrays for the input data and two for the output data, accounting for respectively 1587 and 794 MiB. The pointer graph version however requires about 4 million allocations to complete the graph. On both USM shared and USM host the allocation and free time are dramatically increased when going from the flat version to the pointer graph version, while the fill and kernel times are only affected by a mere 1.5 factor at most. This shows that a pointer graph is extremely expensive as long as many small allocations are required. For a comparison with the CPU-only world, going from a flat 1D structure to a pointer graph has a very little impact on the performance of this very same program, except for the free step which is much longer.

\begin{figure}[h]
    \centering
    
    \begin{tabular}{||c c c c c c||} 
    \hline
    & alloc & fill & kernel & free & sum \\ [0.5ex]
    \hline\hline
    shared flat & 0.09 & 838 & 452 & 266 & 1556 \\
    \hline
    shared ptr & 89798 & 1148 & 738 & 34401 & 126085 \\
    \hline
    host flat & 724 & 563 & 11505 & 406 & 13198 \\
    \hline
    host ptr & 64736 & 654 & 15165 & 23835 & 104390 \\
    \hline
    glibc flat & 269 & 562 & 1821 & 5 & 2657 \\
    \hline
    glibc ptr & 397 & 573 & 2073 & 228 & 3271 \\
    \hline

    \end{tabular}

    \caption{Comparison between USM shared with a few big flat arrays and USM shared using a pointer graph. Replacing the usual malloc with a sycl::malloc\_shared may produce a working program, but using many small allocations comes at a huge cost. Here, we used 4 big arrays in the flat scenario and about 400\_000 arrays and pointers in the pointer graph run.}
    \label{fig:sparseccl_ptr}
\end{figure}

\section{Conclusions}
Conclusion ici.


\section*{References}
\begin{thebibliography}{9}
\bibitem{bibliotemplate} Une belle bibliographie bien formattée ici.
\bibitem{sycl} https://www.khronos.org/registry/SYCL
\bibitem{hipsycl} https://github.com/hipSYCL
\bibitem{sparseccl} A. Hennequin, B. Couturier, V. V. Gligorov and L. Lacassagne, "SparseCCL: Connected Components Labeling and Analysis for sparse images," 2019 Conference on Design and Architectures for Signal and Image Processing (DASIP), 2019, pp. 65-70, doi: 10.1109/DASIP48288.2019.9049184.
\bibitem{iopartnum} IOP Publishing is to grateful Mark A Caprio, Center for Theoretical Physics, Yale University, for permission to include the {\tt iopart-num} \BibTeX package (version 2.0, December 21, 2006) with  this documentation. Updates and new releases of {\tt iopart-num} can be found on \verb"www.ctan.org" (CTAN). 
\end{thebibliography}


\newpage

====== VIEUX BLABLA BIENTÔT SUPPRIMÉ ====== \linebreak
====== VIEUX BLABLA BIENTÔT SUPPRIMÉ ======  \linebreak
====== VIEUX BLABLA BIENTÔT SUPPRIMÉ ======  \linebreak
============================================================================================= \linebreak
============================================================================================= \linebreak
============================================================================================= \linebreak
============================================================================================= \linebreak




\begin{center}
\begin{tabular}{||c c c c c c||} 
\hline
& alloc & copy/access & kernel 1 & kernel 2 & free \\ [0.5ex]
\hline\hline
device & 7 & 671 & 17 & 11 & 15 \\
\hline
shared & 0.1 & 981 & 24 & 23 & 169 \\
\hline
host & 1935 & 404 & 525 & 3 & 1014 \\
\hline
accessors & 0.0 & 0.02 & 701 & 11 & 5 \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{||c c c c c c c||} 
\hline
& alloc & copy & ker 1 & ker 2 & free & \\ [0.5ex]
\hline\hline
device & 7 & 671 & 17 & 11 & 15 & device \\
\hline
shared & 0.1 & 981 & 24 & 23 & 169 & shared \\
\hline
host & 1935 & 404 & 525 & 3 & 1014 & host \\
\hline
accessors & 0.0 & 0.02 & 701 & 11 & 5 & accessors\\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{||c c c c c||} 
\hline
& device & shared & host & accessors \\ [0.5ex]
\hline\hline
alloc & 7 & 0.1 & 1935 & 0.0 \\
\hline
copy & 671 & 981 & 404 & 0.02 \\
\hline
ker1 & 17 & 24 & 525 & 701 \\
\hline
ker2 & 11 & 23 & 3 & 11 \\
\hline
free & 15 & 169 & 1014 & 5 \\
\hline
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{||c c c||} 
\hline
& s. direct & s. copy \\ [0.5ex]
\hline\hline
alloc & 0.1 & 0.1 \\
\hline
write & 1659 & 981 \\
\hline
ker\textsubscript{1} & 1383 & 24 \\
\hline
ker\textsubscript{2} & 31 & 32 \\
\hline
free & 654 & 169 \\
\hline
\end{tabular}
\end{center}


\begin{tabular}{||c c c c c c||} 
\hline
& alloc & write & ker\textsubscript{1} & ker\textsubscript{2} & free \\ [0.5ex]
\hline\hline
s. direct & 0.1 & 1659 & 1383 & 31 & 654 \\
\hline
s. copy & 0.1 & 981 & 24 & 32 & 169 \\
\hline
\end{tabular}




\includegraphics[width=0.5\textwidth]{ubench_direct.png}

\begin{center}
\begin{tabular}{||c c c c c c||} 
\hline
& alloc & copy/access & kernel 1 & kernel 2 & free \\ [0.5ex]
\hline\hline
s. direct & 0.1 & 1659 & 1383 & 22 & 654 \\
\hline
s. copy & 0.1 & 981 & 24 & 23 & 169 \\
\hline
\end{tabular}
\end{center}
    

Courbe avec USM shared et shared direct seulement. Dire (dans la description de la courbe ?) que USM host direct est identique à USM host copie explicite.

Transition : qu'est-ce qui se passe lorsque que les données sont accédées directement sans copie explicite, laissant à SYCL le soin des mouvements de données.

Benchmarks :
- pas besoin de relancer le microbenchmark je pense, ça pourrait être intéressant de remplir manuellement toutes les cases du tableau et de voir à quel point les performances se dégradent, mais c'est moins essentiel qu'avoir fini la publication à temps. Mais je peux, pour la forme, et après, relancer des benchmarks pour avoir les données les plus propres possibles.
- relancer le graphe de pointeurs avec le ld maximal (100 ne marche pas, mais peut-être que 50 ça fonctionnerait ?)

Ça sera dans SparseCCL :
The main advantage of USM shared is that data can be accessed both from the host and the device. : transition avec le graphe de pointeurs, dire que USM shared permet une utilisation transparente en remplaçant le traditionnel malloc par un malloc\_shared, mais à quel prix ? flat vs graphe de ptr.


Host memory is as expensive as it gets when it comes to allocations and deallocations, most likely due to the device memory being way faster than the host memory. \textit{(Question ouverte, je ne sais pas trop, mais on parle quand-même de mémoire fragmentée NUMA (?) DDR6 sur GPU avec probablement une gestion paresseuse tant que possible, là où on a une gestion par l'OS manifestement pas paresseuse du tout de la mémoire host DDR4, carrément plus loin du CPU. A creuser.)}

% Comm sur question de Hadrien : L'OS est paresseux aussi, de mémoire la stratégie par défaut de Linux est de n'allouer les pages de mémoire d'une allocation malloc qu'au moment où on y accède pour la première fois ("first-touch").



Sera supprimé : \textit{USM host and shared “direct” happens when one directly writes into the SyCL memory from the host, instead of calling a SyCL explicit copy from host memory to SyCL memory. USM host performance is not at all impacted while}  \medbreak 

\textit{Here is a performance analysis using the various SyCL memory models.} 



Data is not already formatted in 1 dimensional arrays so it has to be flattened to make it copyable to the SyCL Unified Shared Memory. This is also necessary in order to pass a pointer to a host contiguous memory allocation for creating the sycl::buffer of the accessors/buffers method. (TODO : moche). During the alloc phase, 

Data has been previously converted into a 1D array on host memory to make it explicitly copyable from host to SyCL memory. [continuer d'ici] Comparison between USM device, shared and host



ça aurait été intéressant de faire pour USM shared : allocation mémoire host, copie explicite SYCL vers USM shared puis kernel et de le comparer à la courbe actuelle USM shared. Porbablement qu'on aurait une allocation plus longue, mais une copie plus rapide (pas tant que ça parce que copie = fill + copie vers USM shared), et un kernel super rapide. 

Dans l'ordre : 
- USM device : allocation host + allocation device + fill host + copie host->device + kernel + copie device -> host + free
- USM shared : allocation shared + fill shared + kernel + free
- accessors  : allocation host + création buffers + fill host + kernel + demande d'accès buffers + free

Du coup :
- alloc
- fill
- copy + kernel
- free

TODO : mettre partor buffers/accessors ou accessors/buffers mais ne pas changer sans cesse !

First is a comparison [Dire que 1) flat arrays pour que ce soit faicle à copier explicitement de la mémoire Host à la mémoire device, et que ce soit facile à exploiter côté GPU 2) voir ce que ça fait lorsqu'on remplace simplement le malloc classique par un malloc\_shared ou \_device : le développement est alors plus aisé parce que la structure des données n'a pas à être changée, mais à quel prix ?]

It is only natural to ask whether 

\textit{kernel 2} for the second one. (TODO : mettre dans le texte) run and demonstrates that some operations are done in a lazy way in comparison with the second kernel run.  Note that \textit{alloc} and \textit{free} do not have to be repeated each time a computation on device is needed, as memory allocations may be reused.


Seulement un tableau pour la comparaison USM shared graphe de pointeurs vs applati, le graphique n'apporte rien et montre juste que USM en applati est totalement applati par la courbe en graphe de pointeurs, un tableau de valeurs est bien plus clair, je pense.

Parler de :
- Comparaison entre USM shared et host en flat
- et de USM shared et host en utilisant un graphe de pointeurs

As seen before, USM shared does a number of things asynchronously. Memory allocation is only done when memory is accessed for the first time. Liberation however seems to be done in a synchronous way. USM device and the accessors-buffers show a very similar performance profile, except the allocation is much more expensive for USM device: here the memory is initialized with specific flags to give the API more room to optimize its usage. The read\_only, write\_only and no\_init flags does not yet exists in the USM world.





\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{SparseCCL_ptr_vs_flat.png}
    \caption{This graphic is blablabla.}
    \label{fig:mesh3}
\end{figure}

The allocation/free cost is huge when using pointer graphs (400 000 small allocations) compared to using regular flat arrays (4 big allocations) for USM shared and host. The kernel in itself does not seem to be heavily impacted. However, USM shared is still much faster in kernel duration compared to USM host as data is used multiple times: the cost of passing through the PCIe bus is paid only once with USM shared, however it is paid each time data is accessed when using USM host.

23h30

\end{document}


