\documentclass[a4paper]{jpconf}
\usepackage{graphicx}
\begin{document}
\title{Comparing SYCL data transfer strategies for tracking use cases }

% \author{Jacky Mucklow}
% \address{Production Editor, \jpcs, \iopp, Dirac House, Temple Back, Bristol BS1~6BE, UK}
% \ead{jacky.mucklow@iop.org}

\author{S Joube$^{1, 2}$, H Grasland$^1$,
D Chamont$^1$, and E Brunet$^2$}
\address{$^1$ Université Paris-Saclay, CNRS/IN2P3, IJCLab, 91405 Orsay, France}
\address{$^2$ Samovar, Télécom SudParis, Institut Polytechnique de Paris, rue Charles Fourier, 91011 Evry, France}
\ead{joube@ijclab.in2p3.fr}

\begin{abstract}

%Like many particle physics programs, the ACTS particle tracking library requires heavy calculations. 

The aim of this work is to compare the performance and ease of programming of the various data transfer strategies provided by SYCL 2020: buffers/accessors on the one hand and the different storage types exposed by Unified Shared Memory (USM) on the other hand. We measured the relative performance of USM exclusively located either on the host (USM host) or on the device (USM device), or automatically managed and moved (USM shared). We also tried to evaluate the impact of formatting data in a GPU-friendly manner rather than using a regular pointer-based structure in which the C++ allocator was replaced by the sycl allocator sycl::malloc. We first made a memory intensive microbenchmark to test the SYCL memory models with a simple access pattern. We then switched to a real use-case provided by traccc, a research project associated to the ACTS particle tracking library. The algorithm of interest is SparseCCL, a clustering algorithm used on the first step of track reconstruction. For consistency, all tests were made on a single hardware architecture: a recent server having a discrete GPU with dedicated VRAM, representative of the hardware currently used by the ACTS team.

\end{abstract}

\section{Introduction}

\textbf{[Rapide mise en contexte générale] et [Problématique], contexte, motivation, application}
The increased use of accelerators for scientific computing, together with the increased variety of hardware involved, induces a need for performance portability between at least CPUs and GPUs. In the C/C++ community, OpenCL was a low level first step, which is now superseded by SYCL for C++ use cases such as the ACTS library. ACTS is an experiment-independent toolkit for charged particle track reconstruction in high energy physics experiments implemented in modern C++.
\textbf{[Présentation de SYCL]}
SYCL is a royalty-free, cross-platform abstraction layer that enables code for heterogeneous processors to be written using standard ISO C++17 with the host and kernel code for an application contained in the same source file.
\textbf{[Présentation du hardware utilisé]}
Every benchmark was ran on the same architecture: a Debian 11.0 stable based server with 512 GB DDR4-2400, an AMD EPYC 7502 CPU with 32 cores, 64 threads, an NVidia Quadro RTX 5000 having 16 GB GDDR6 and a theoretical memory bandwidth of 448 GB/s, a PCIe 3.0 x16 bus with a theoretical bandwidth of 16 GB/s. Observed CPU-GPU data transfer was only 12 GB/s, but according to Nvidia experts, this may be expected. \textbf{[Présentation du logiciel utilisé]} The SYCL code was ran by HipSYCL 0.9.1-20210702 amd64, using the cuda 10.1 backend.

% TODO: Ajouter ref + version hipSYCL (et bien insister que c'est hipSYCL, performances des buffers/acesseurs potentiellement fortement dépendantes de l'implémentation utilisée)

\section{Microbenchmark}
We made a simple microbenchmark to give a first approximation of the relative performance of the SYCL memory models on a single hardware architecture, and compare it with what is theoretically expected. This microbenchmark is a simple reduction on GPU, heavily I/O bound, computing partial sums of about 1.6 billion randomly generated 4-byte floats without synchronisation between threads. Two kernels were launched each time, one after the other, to identify what is done in a lazy way. \medbreak 

\subsection{Explicit copy}

\begin{figure}[h]
    \centering
    %\includegraphics[width=0.9\textwidth]{ubench_explicit4.png}
    \includegraphics[width=1.0\textwidth]{ubench_copy25_d.png}
    %\includegraphics[width=0.9\textwidth]{ubench_copy23.png}
    
    %\begin{center} <- décommenter pour prendre plus de place et du coup aérer plus
    \begin{tabular}{||c c c c c c c c||} 
    \hline
    & alloc & copy & ker\textsubscript{1} & ker\textsubscript{2} & dealloc & total & \\ [0.5ex]
    \hline\hline
    device & 7 & 671 & 17 & 17 & 15 & 726 & device \\
    \hline
    shared & 0.1 & 981 & 24 & 32 & 169 & 1204 & shared \\
    \hline
    host & 1935 & 404 & 525 & 530 & 1014 & 4409 & host \\
    \hline
    accessors & 0.0 & 0.02 & 701 & 17 & 5 & 723 & accessors \\
    \hline
    \end{tabular}
    %\end{center} <- décommenter pour prendre plus de place et du coup aérer plus

    
    \caption{Elapsed time in milliseconds for the main stages of the microbenchmark, using an explicit copy from host native memory to SYCL USM on the one hand and the buffers/accessors model on the other. \textit{alloc} represents the SYCL USM memory allocation and buffers declaration. During the \textit{copy} step data is explicitly copied to the SYCL memory for USM, and buffers are linked to the data they will manage on the host. \textit{kernel 1} stands for the first kernel. \textit{kernel 2} is launched just after the first one has completed, and works with the same dataset. \textit{dealloc} is the final deallocation of the SYCL memory. }
    \label{fig:microbenchmark}
\end{figure}

\textit{USM device}, \textit{USM shared} and \textit{USM host} curves shown in Figure \ref{fig:microbenchmark} are made with an explicit SYCL memory copy (sycl::memcpy) from host native memory to the USM handled by SYCL. The input size for the computation is 6.44 GB (6 GiB), while the output size is 50 MB (48 MiB). The copy for USM device takes 671 ms to complete, accounting for an observed speed of about 9.6 GB/s if we consider the overheads duration to be insignificant compared to the copy duration itself. Its kernel is processed in 17 ms, leading to a deduced memory bandwidth for the GPU of about 380 GB/s, relatively close to the theoretical 448 GB/s of this GPU, as the microbenchmark is heavily memory-bound. Given that the USM host kernel only takes about 520 ms (12.4 GB/s), its usage seems to fit the single-access-to-data use-case if no copy is needed and data is handled and allocated through sycl::malloc\_host instead of the regular malloc. This avoids paying for the copy overhead of USM device, provided that the SYCL host memory is heavily reused to compensate for its huge allocation and deallocation costs. \medbreak 

The buffers/accessors curve has a very similar performance profile to USM device, except the memory copy is performed in a lazy way the first time data is accessed (GPU kernel run 1). Its first kernel takes about 701 ms, roughly equivalent to the 671 ms of SYCL explicit copy plus the 17 ms of kernel time taken by USM device. Its allocation time is nearly zero as only the sycl::buffer wrapper objects are created. The "copy" time also is very small as it only consists of passing a pointer to the sycl::buffer. As said before, the data is only copied when needed, that is during the first kernel run. \medbreak 

USM shared also has a very low allocation time, but a slightly higher copy time compared to USM device. The real allocations on device are done during the copy phase. The internal logic needed by USM shared also has a significant cost paid on the dealloc SYCL memory step. \medbreak 

The buffers/accessors approach is the oldest and highest-level way of managing data, as it relies on a task dependency graph, leaving data transfer scheduling entirely up to the SYCL implementation. It is comparatively efficient, provided data already appears in contiguous arrays that can easily be passed to sycl::buffers as pointers. As far as USM goes, USM device is the most efficient way of dealing with a heavily memory-bound program, with the same constraints on data format than the buffers/accessors technique: to make an efficient sycl::memcpy, data must be stored in large contiguous arrays. The one downside to USM device is that data cannot be larger than device memory, and has to be manually fragmented if too large, an issue that no other method has \textit{[TODO : à vérifier expérimentalement]}. There is no limit to data size for USM host, shared and the buffers/accessors, except for the physical available memory on the host. USM host has a large allocation and deallocation cost which must be amortized through allocation reuse, and is only useful for data accessed only once by the device and generated inside SYCL USM host memory, as opposed to being copied from a regular CPU memory allocation. \medbreak 

\subsection{Direct access to USM}

\begin{figure}[h]
    \centering
    
    \includegraphics[width=1\textwidth]{ubench_write25_d.png}
    \medbreak
    %\begin{center}
    \begin{tabular}{||c c c c c c c c||} 
    \hline
    & alloc & copy/write & ker\textsubscript{1} & ker\textsubscript{2} & dealloc & total & \\ [0.5ex]
    \hline\hline
    s. direct & 0.1 & 1659 & 1383 & 31 & 654 & 3727 & s. direct \\
    \hline
    s. copy & 0.1 & 981 & 24 & 32 & 169 & 1204 & s. copy \\
    \hline
    \end{tabular}
    %\end{center}

    \caption{Elapsed time in milliseconds for the main stages of the microbenchmark, comparing an explicit SYCL copy from host native memory to USM shared memory, and a direct access to USM shared memory without the SYCL explicit copy. The meaning of the labels is exactly the same as with Figure \ref{fig:microbenchmark} except data is copied to the SYCL memory during the \textit{copy/write} step for USM shared copy and directly written to for USM shared direct.}
    \label{fig:microbenchmark_write}
\end{figure}

As seen before, the main advantages of USM shared and host over USM device are that they can handle data way bigger that the device memory and that pointers remain valid from both host and device, making data always accessible regardless of whether it is accessed from the device or the host. Figure \ref{fig:microbenchmark} shows an explicit copy from host native memory to USM. Figure \ref{fig:microbenchmark_write} compares the previous USM shared behaviour with the one obtained when data is directly written to the USM shared memory from the host, instead of being copied with sycl::memcpy. For readability, USM host was not added to the graph as it has the exact same performance profile when explicitly copied or directly written to. \medbreak 

Figure \ref{fig:microbenchmark_write} shows that direct access to USM shared memory is very costly: the writing time to the SYCL memory is multiplied by 1.6 and the first kernel time by 55, while the second kernel time is not affected. By profiling the execution of this program using Nvidia's Visual Profiler (nvvp), we found that the memory copy still goes on in an asynchronous way until the first kernel has completed. Automatic data migration proves to be highly inefficient in this case, as the SYCL implementation does not know we need to access memory in a respectively read-only and write-only mode. This results in a large number of page faults as data is accessed from the device, that only finishes during the first kernel run once the whole dataset has been accessed by the kernel and data has been entirely moved to the device memory. The second kernel run has a very close performance profile to the explicit sycl::memcpy. Here, the whole input and output data buffers could be stored entirely in the device memory as the comparison with USM device would not have been possible otherwise. The \textit{dealloc SYCL} stage shows that the synchronisation mechanisms used by USM shared, together with the API not knowing whether we wanted a \textit{read-only} or a \textit{write-only} buffer, makes deallocation very expensive. Most likely deallocation is more expensive in the direct access case because a host memory mapping had to be set up and must now be torn down. \medbreak 


\section{SparseCCL}

SparseCCL is an algorithm specifically designed for HEP tracking. It is in charge of the very first step of track reconstruction: the connected component labeling also known as clustering. Its memory access pattern is more complex than that of our microbenchmark, and also represents a much more realistic usage of the SYCL API. This section still aims to compare the relative performance of the various USM allocation types, together with the buffers/accessors technique.

\subsection{SparseCCL with flat arrays}

First we chose to structure the memory in a relatively device-friendly fashion. Data is flattened into large contiguous arrays, easily copyable to the device. Because the initial data is not already flattened into an array, the allocation (alloc) step depicted in Figure \ref{fig:sparseccl_flat} is quite different from the one shown on Figure \ref{fig:microbenchmark}. For USM device and the buffers/accessors, a host memory allocation is needed to flatten the data and then either explicitly copy it to the USM device memory or pass it as a pointer to a sycl::buffer. Therefore, the alloc phase displays the native host memory allocation for both USM device and the buffers/accessors, in addition to the device memory allocation for USM device and the (still insignificant) creation time of sycl::buffer wrappers objects. The fill phase represents the memory being filled with the data of interest: data is put into the flat host memory for USM device and the buffers/accessors, while data is directly written to the USM shared memory. During the \textit{copy+kernel} phase memory is copied from host to USM device and then the kernel starts, while the kernel starts directly for USM shared and the buffers/accessors as SYCL is in charge of the implicit data migration. Finally, memory is deallocated during the dealloc step. The need to allocate and fill some host memory for the buffers/accessors and for USM device greatly impairs performance, making USM shared much more competitive than previously depicted. Instead of accessing device local memory, or even caches, an I/O request on the PCIe bus is issued each time the kernel needs to access memory, leading to extremely poor kernel performance.

\begin{figure}[h]
    \centering
    %\includegraphics[width=0.8\textwidth]{SparseCCL_flat.png}
    %\includegraphics[width=0.8\textwidth]{sparseccl_flat_sandor_ld100_reduced.png}
    %\includegraphics[width=1\textwidth]{sparseccl_onlyFlat2.png}
    \includegraphics[width=1\textwidth]{sparseccl_onlyFlat3_dl.png}
    
    \medbreak 
    \begin{tabular}{||c c c c c c c||} 
    \hline
    & alloc & fill & copy+kernel & dealloc & total & \\ [0.5ex]
    \hline\hline
    device & 685 & 575 & 332 & 311 & 1903 & device \\
    \hline
    shared & 0.09 & 838 & 452 & 266 & 1556 & shared \\
    \hline
    accessors & 276 & 574 & 392 & 13 & 1255 & accessors \\
    \hline
    host & 719 & 564 & 12040 & 347 & 13670 & host \\
    % host & 724 & 563 & 11505 & 406 & 13198 \\
    \hline
    \end{tabular}
    
    \caption{Elapsed time in milliseconds for the main stages of the SparseCCL memory management. The necessary allocations are made during the alloc phase. Data is flattened into big arrays on the fill step. The kernel+copy phase is where the explicit copy and kernel takes place for USM device, while only the kernel runs for USM shared and the buffers/accessors. Finally, the memory is deallocated during the dealloc phase. For readability, USM host has not been shown as its kernel takes a much longer time to complete, due to many accesses to the same data.}
    \label{fig:sparseccl_flat}
\end{figure}

\subsection{SparseCCL: flat arrays versus pointer graph}

Flattering data to make it usable by any device is not the only option. Thanks to the SYCL USM shared and host allocation types, pointers can be used both on device and on the host as previously stated. It is thus possible for any CPU-only program to keep the same data structure and make it run on any device supported by a SYCL implementation and backend, provided some changes are made to the computation algorithms to fully take advantage of the device architecture in use. To make the data accessible by the host and the device, it is only necessary to change the regular c++ allocator such as malloc or new, to a sycl::malloc\_host or sycl::malloc\_shared allocator. \medbreak 

The performance implications of such a change are shown in Figure \ref{fig:sparseccl_ptr}. The flat version only requires 4 arrays to be allocated with SYCL USM shared, host or with a regular malloc: two arrays for the input data and two for the output data, accounting for respectively 1664 MB and 833 MB. The pointer graph version, however, requires about 4 million allocations to complete the graph. On both USM shared and USM host the allocation and deallocation time are dramatically increased when going from the flat version to the pointer graph version, while the fill and kernel times are only affected by a mere 1.5 factor at most. This shows that a pointer graph is a very costly data layout as long as many small allocations are required. In a CPU-only implementation of the same algorithm, going from a flat 1D structure to a pointer graph has a much smaller impact on performance, except for the deallocation step which is much longer. To summarize, while replacing the usual malloc with a sycl::malloc\_shared may produce a working program, using many small allocations comes at a large cost.

\begin{figure}[h]
    \centering
    \begin{tabular}{||c c c c c c c||} 
    \hline
    & alloc & fill & kernel & dealloc & total & \\ [0.5ex]
    \hline\hline
    shared flat & 0.09 & 838 & 452 & 266 & 1556 & shared flat \\
    \hline
    shared ptr & 89798 & 1148 & 738 & 34401 & 126085 & shared ptr \\
    \hline
    host flat & 719 & 564 & 12040 & 347 & 13670 & host \\
    \hline
    host ptr & 64736 & 654 & 15165 & 23835 & 104390 & host ptr \\
    \hline
    cpu flat & 277 & 574 & 1866 & 5 & 2722 & cpu flat \\
    \hline
    cpu ptr & 397 & 573 & 2073 & 228 & 3271 & cpu ptr \\
    \hline
    \end{tabular}

    \caption{Comparison between the use of a flat data structure consisting of few big flat arrays and a pointer graph. The time unit is milliseconds.}
    \label{fig:sparseccl_ptr}
\end{figure}

\section{Conclusions}

SYCL is an emerging standard for heterogeneous computing with a sophisticated memory management API. We showed that on a simple microbenchmark with data already formatted in a very few big arrays, USM device and the buffers/accessors were clearly the most efficient way of handling data. Regarding USM shared, an explicit copy by a sycl::memcpy was way faster than directly filling this memory from the host, but made only sense when data was already formatted in a few flat arrays. The SparseCCL use-case showed the performance gap between USM shared, device and the buffers/accessors to be much more subtle as the data needed to be flattened to work in the same way it did with the microbenchmark. Finally, even if replacing the usual C++ allocator with a SYCL sycl::malloc\_shared/host makes for a functioning program, the SYCL allocator proved to have very poor performance when asked to perform a large number of small allocations. An issue the CPU allocator provided by glibc does not seem to have, possibly due to differences in PCIe communication patterns.

\section{Future work}

Due to time constraints, this preliminary work focused on a single hardware architecture and a single implementation (hipSYCL) and backend (Nvidia Cuda 10.1). A broader list of devices, implementations and backends will be studied in the near future, such as the Intel OneAPI Data Parallel C++ implementation of the SYCL standard, and more devices like AMD discrete graphic cards and Intel and AMD integrated graphic processors, from different generations. Other use-cases will also be tested, including next steps of the ACTS track reconstruction chain such as seeding, to explore a broader range of data access patterns.

\section*{References}
\begin{thebibliography}{9}
\bibitem{bibliotemplate} Une belle bibliographie bien formattée ici.
\bibitem{sycl} https://www.khronos.org/registry/SYCL
\bibitem{hipsycl} https://github.com/hipSYCL
\bibitem{sparseccl} A. Hennequin, B. Couturier, V. V. Gligorov and L. Lacassagne, "SparseCCL: Connected Components Labeling and Analysis for sparse images," 2019 Conference on Design and Architectures for Signal and Image Processing (DASIP), 2019, pp. 65-70, doi: 10.1109/DASIP48288.2019.9049184.
\bibitem{iopartnum} IOP Publishing is to grateful Mark A Caprio, Center for Theoretical Physics, Yale University, for permission to include the {\tt iopart-num} \BibTeX package (version 2.0, December 21, 2006) with  this documentation. Updates and new releases of {\tt iopart-num} can be found on \verb"www.ctan.org" (CTAN). 
\end{thebibliography}

\newpage


version 2022-02-21 18h18

\end{document}


